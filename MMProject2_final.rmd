---
title: "Data Science Project -  A Review of the Relationship Between Song Features and Its Relative Popularity With Respect to Time"
author: "Memes and Music: \nRich Gude \nSiwei Yang \nJunhe Zhang \nKrystal Payton"
date: "April 22, 2020"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: true
  pdf_document:
    toc: yes
---

<style type="text/css">
.main-container {
  
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = T, message = F)
# knitr::opts_chunk$set(warning = F, results = F, message = F)
# knitr::opts_chunk$set(include = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# 'scipen': integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than 'scipen' digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r basicfcn, include=F}
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
# note that using this function requires quotes around the package name, as you would when installing packages.
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
# unload/detact package when done using it
# detach_package = function(pkg, character.only = FALSE) { if(!character.only) { pkg <- deparse(substitute(pkg)) } search_item <- paste("package", pkg, sep = ":") while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } }
```

```{r outlierKD2, include = F}
# Fix outliers
outlierKD2 <- function(df, var, rm=FALSE) { 
    #' Original outlierKD functino by By Klodian Dhana,
    #' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
    #' Modified to have third argument for removing outliers instead of interactive prompt, 
    #' and after removing outlier, original df will not be changed. The function returns the new df, 
    #' which can be saved as original df name if desired.
    #' Check outliers, and option to remove them, save as a new dataframe. 
    #' @param df The dataframe.
    #' @param var The variable in the dataframe to be checked for outliers
    #' @param rm Boolean. Whether to remove outliers or not.
    #' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
    #' @examples
    #' outlierKD2(mydf, height, FALSE)
    #' mydf = outlierKD2(mydf, height, TRUE)
    #' mydfnew = outlierKD2(mydf, height, TRUE)
    dt = df # duplicate the dataframe for potential alteration
    var_name <- eval(substitute(var),eval(dt))
    na1 <- sum(is.na(var_name))
    m1 <- mean(var_name, na.rm = T)
    par(mfrow=c(2, 2), oma=c(0,0,3,0))
    boxplot(var_name, main="With outliers")
    hist(var_name, main="With outliers", xlab=NA, ylab=NA)
    outlier <- boxplot.stats(var_name)$out
    mo <- mean(outlier)
    var_name <- ifelse(var_name %in% outlier, NA, var_name)
    boxplot(var_name, main="Without outliers")
    hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
    
    # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
    # if(response == "y" | response == "yes"){
    if(rm){
        dt[as.character(substitute(var))] <- invisible(var_name)
        #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
        cat("Outliers successfully removed", "\n")
        return(invisible(dt))
    } else {
        cat("Nothing changed", "\n")
        return(invisible(df))
    }
}
```

# Executive Summary:

The purpose of this study it to identify: Does the key, valence, tempo, or other musical attributes change between popular American songs, and not popular songs for contrast, from the 1960's versus songs from the early 2000's?  Musical attributes under consideration are gathered from the Spotify Audio Analysis tool that records basic musical metrics, such as tempo, key, and mode, as well as more advanced, proprietary metrics, such as energy, valence, and danceability.  Popular songs from each decade are identified as those that have appeared on the Billboard Hot 100, published by Billboard magazine.  This study utilizes three predictive models, K-Nearest Neighbor, Decision Tree, and Logistic Regression, to predict the binary "popularity" feature, popular or not popular.  If each model has greater than 50% accuracy (signifying it is superior to random guessing), then it can be determined that musical attributes *do* have an effect on song popularity, and the features that appear in each model are those that have the most significant impact.

# Purpose:

The purpose of this study is to analyse the affect of music attributes, such as tempo, key, mode, etc, on the popularity of songs with respect to time.  Specifically, the SMART question for this study is: Does the key, valence, tempo, or other musical attributes change between popular songs, and not popular songs for contrast, in the 1960's versus songs from the early 2000's?

The methodology by which this will be accomplished is to analyze song attribute data collected from Spotify using their application programming interface (API) and compare these attribute values between "popular" and not-"popular" songs from two decades, the 1960's and the 2000's.  Spotify API attribute data collects the following features for every song, or "track", under review: track, artist, key, mode, valence, tempo, "energy", "liveness", and "danceability".  The popularity of each song will be a binary attribute: A song will be considered popular if it appears in the Billboard Hot 100, a weekly top list of the popular tracks from each week measured by Billboard Magazine.  Billboard Magazine is the widely considered standard for song popularity and has existed since the 1960s through today and, so, represents a consistent standard for measuring song popularity through time.

# A Discussion of Basic Music Theory

Music, as an art, has a long and varied history depending on the era and geographical space in which an individual may be concerned.  While the purpose of this study is to identify the change of popular music, at least over a short period since what is 50 years to the entire timeline of the human species over which music in some form is sure to have been evident, the entire history of what music is composed and how it is written or expressed will not be discussed herein.  Instead, this study is focused on contemporary, American-produced songs, and so, certain objective measurements of song qualities, namely tempo, key, and mode, can be used to compare and contrast songs both across the musically spectrum, from say jazz to rock-and-roll, and through time.

Tempo, key, and mode are relatively basic concepts in music theory.  The beat is the basic unit of time in music, the pace at which the music pulses; it is, essentially, when a listener would tap their toe during a song.  **Tempo** is the speed or pace of a given song and is often measured in beats per minute.  Certain genres of songs are defined by high tempos, like electric-dance music (EDM), but most genres vary considerably in their tempo.  Pitch is the audio frequency at which individual notes within the song are heard by the listener.  In general terms, the **key** of a song is the average pitch of a song, or the pitch around which a song fluctuates; performing a song in a higher key means that all of the notes of a song are increased at a level commiserate with the change in key (a major key change from "C'" to "D", one pitch, would change the pitch of all notes in the song up pitch as well).  Along the same concept as key, **mode** is the interval at which pitches are expressed in a song and is expressed as either "minor" or "major".  The key and mode are often expressed together when describing the pitch qualities of a song, such as "F major" or "D minor".  From an audio perspective, "major" keys are more often associated with and evoke happy or bright melodies, whereas "minor" keys may sound melancholic.

The concepts discussed here are general introduction to the features that will be discussed as part of this study's dataset, and the format of this study herein, namely a written report, does not support audio examples, which would be necessary for any comprehensive understanding of music theory.  Additional discussion of tempo, key, and mode, with audio examples for the differences between keys and modes, can be found [here](https://www.youtube.com/watch?v=rgaTLrZGlk0).

# Data Selection:

As identified in the methodology statement, the data in this study is pulled from two corporate sources, Spotify Technology S.A. (Spotify) and the Billboard-Hollywood Media Group.  Spotify is a popular online streaming service for music, videos, and podcasts.  Spotify provides an "Audio Analysis" of a musical track that describes the structure of the track and its musical content, including tempo, key, and mode discussed above, in addition to more sophisticated musical metrics relating multiple core concepts of music.  The advanced metrics that will be tracked from Spotify Audio Analysis in this study are valence, "danceability", "energy", and "liveness"; these metrics are discussed further below.  The Billboard-Hollywood Media Group owns and produces the "Billboard" magazine.  This publication is famous for the Billboard Hot 100 list, a weekly-published list that identifies the most popular, American songs of the week, based on sales and digital downloads and streams.  The Billboard Hot 100 has been in publication since 1958 and establishes an objective standard for identifies popular songs for the purposes of this study.

Based on the source from which popular songs are determined, the conclusions of this study should only apply to songs produced or with a large-commerical release in the United States of America.  Song features for popular songs based on similar metrics of sales and digital views from other countries may vary from the results reported herein.

The data analyized herein was pulled from the Kaggle database website, from a dataset collected and produced by user, Farooq Ansari.  The specific dataset can be found [here](https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset).

# Data Background:

As stated previously, the Spotify Audio Analysis records multiple features for each track in its extensive collection of music titles.  Some of these features, such as tempo, key, and mode, have strict definitions within the music community and are, otherwise, clear, quantifiable variables (e.g., tempo is measured in beats per minute).  Other features do not have strict definitions within the music community and/or do not have a clear, quantifiable standard that can be measured within each track.  The following features are ranked in Spotify's analyses on a scale of 0 to 1 and will be considered for analysis in this study: [^1]
- **Valence** describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).  For perspective: from the 2000's data set, one popular song with a high valence (0.965) is OutKast's ["Hey Ya"](https://open.spotify.com/track/2PpruBYCo4H7WOBJ7Q2EwM?si=3b7dll0ITZ-H5FsIu-qGig), while a song with low valence (0.0356) is deadmau5's ["Strobe"](https://open.spotify.com/track/31NiyZrUd1r4icY7xkvnWv?si=przVh9EjRgG-oyH5rInY9A).
- **Danceability** describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. For perspective: from the 2000's data set, one popular song with a high danceability (0.956) is Nelly's ["Hot in Herre"](https://open.spotify.com/track/04KTF78FFg8sOHC1BADqbY?si=i8f_wIreTsu34rIpSX4PDA), while a song with low danceability (0.0356) is Venom's ["Black Metal"](https://open.spotify.com/track/3yNoEJifUJdly8ucYoWRwl?si=15rDImzOTkSfJlpggbrPLw). 
- **Energy** represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. For perspective: from the 2000's data set, one popular song with a high energy (0.991) is Fatboy Slim's ["The Rockafeller Skank"](https://open.spotify.com/track/7mCQK9YB25WZw1saUjfL4e?si=46Rwt-TMQgusLo_Wjm3o0g), while a song with low energy (0.0013) is Alvin Curran's ["Inner Cities II"](https://open.spotify.com/track/4De0j0rVNmezk0EXPzOtwZ?si=ShNPfVWrQfKzZPiBlxKS-g).
- **Liveness** detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. For perspective: from the 2000's data set, one popular song with a high liveness (0.959) is Metallica's ["No Leaf CLover - Live"](https://open.spotify.com/track/0n4AllHzf3ma4ki20Y9h00?si=d3-JKQO9SOeTVxMzNhOYYQ), while any studio-produced song would have a low liveness.

The full analysis of these features, namely the methods or calculations by which their values are determined, are not released by Spotify, potentially due to intellectual property reasons.

The Billboard Hot 100 has several component charts that contribute to the overall calculation of the Hot 100 each week. The most significant components are: [^2]
- **Hot 100 Airplay**: (per Billboard) approximately 1,000 stations, "composed of adult contemporary, R&B, hip hop, country, rock, gospel, Latin and Christian formats, digitally monitored twenty-four hours a day, seven days a week. Charts are ranked by number of gross audience impressions, computed by cross-referencing exact times of radio airplay with Arbitron listener data." 
- **Hot Singles Sales**: (per Billboard) "the top selling singles compiled from a national sample of retail store, mass merchant and internet sales reports collected, compiled, and provided by Nielsen SoundScan." The chart is released weekly and measures sales of physical commercial singles. With the decline in sales of physical singles in the US, many songs that become number one on this chart often do not even chart on the Hot 100.
- **Digital Songs**: Digital sales are tracked by Nielsen SoundScan and are included as part of a title's sales points.
- **Streaming Songs**: a collaboration between Billboard, Nielsen SoundScan and National Association of Recording Merchandisers which measures the top streamed radio songs, on-demand songs and videos on leading online music services.

From these varied and independent sources, the Billboard Hot 100 represents a nuetral and storied arbiter of the objective popularity of songs from 1958 thorough the present, including all songs to be evaluated from the 1960s and early 2000s.

# Data Preprocessing

The Spotify Audio Analysis stores up to 42 music features for each track.  For the purposes of this study, only the following variables will be analyzed and reviewed:

``` {r DataImport_Gude, include = F, results = F}
loadPkg('tidyverse')    # For general functions
loadPkg('plyr')

# Import 1960's and 2000's data as separate dataframes
data1960 <- subset(data.frame(read_csv('dataset-of-60s.csv')), select = c('track', 'artist', 'tempo', 'key', 'mode', 'valence', 'danceability', 'energy', 'liveness', 'target'))
data2000 <- subset(data.frame(read_csv('dataset-of-00s.csv')), select = c('track', 'artist', 'tempo', 'key', 'mode', 'valence', 'danceability', 'energy', 'liveness', 'target'))
# Change certain columns to factors as necessary
data1960$target <- as.factor(data1960$target)
data2000$target <- as.factor(data2000$target)

data1960_X <- subset(data1960, select = c('tempo', 'key', 'valence', 'danceability', 'energy', 'liveness', 'mode'))
data2000_X <- subset(data2000, select = c('tempo', 'key', 'valence', 'danceability', 'energy', 'liveness', 'mode'))

# Create a dataframe of just popular songs with a new variable for year to distinguish popular songs by year
dataPopular <- bind_rows(mutate(subset(data1960,target ==1), year=1960), mutate(subset(data2000,target ==1), year=2000))
dataPopular$year <- as.factor(dataPopular$year)
```

**Track Variables of Interest:**

1. Track: the name of the track
2. Artist: the name of the artist
3. Tempo: a float value for the beats per minute of the song 
4. Key: an integer mapping of the standard pitch-class notation where: 0 = C key, 1 = C*#*/D*b*, 2 = D, and so on in rising key fashion up to 11 = B key.
5. Mode: an integer mapping for major (1) and minor (0) keys
6. Valence: a float value between 0 and 1 for the relative valence of the track (discussed in the Data Background)
7. Danceabilty: a float value between 0 and 1 for the relative danceability of the track (discussed in the Data Background)
8. Energy: a float value between 0 and 1 for the relative energy of the track (discussed in the Data Background)
9. Liveness: a float value between 0 and 1 for the relative liveness of the track (discussed in the Data Background)

For the purposes of data processing: key will be treated as a numeric value as the scaling nature of keys is suitable for modeling herein - each whole number change in key generally transposes to a constant change on the pitch scale, for C to C*#*/D*b* to D across numerals 0, 1, and 2 for example - and mode will be treated as a categorical (factor) value.  In preprocessing the data, all of the variables from the Spotify data not listed above were eliminated from the study dataset.

The final feature, **Target**, was computed from the collective data from Billboard's Hot 100 list as is represented as a categorical (factor) variable.  Any track listed in the Billboard Hot 100 during the respective decade from which a track was released is given a value of 1, and any track not list at any point in the Billboard Hot 100 is given a 0.

This study will examine the change in music metrics affecting popularity with respect to time.  For this purpose, this study will analyze two datasets, one with songs released during the 1960s, containing `r nrow(data1960)` songs, and the other with songs released from the 2000s, containing `r nrow(data2000)` songs.  Each dataset is composed of an equal number of popular and not popular songs, `r nrow(subset(data2000,target == 1))` popular and not popular songs from the 2000s and `r nrow(subset(data1960,target == 1))` popular and not popular songs from the 1960s.  For every song that appears in the Billboard Hot 100 list from their respective decade (with a target value of 1), another song from the same decade, not appearing in the Hot 100 was chosen to fill in the dataset for analytics and review.  Non-popular songs that were chosen from American-produced songs at random from across music genres with the attempt to capture a suitable mix of music qualities.

A summary of the track musical features and their values from the entire 2000's and 1960's datasets are presented below:

```{r xkablesummary_KP, include=F}
#loadPkg('Rtools')
loadPkg('xtable')
loadPkg('kableExtra')
loadPkg('stringi')

xkabledply = function(smmry, title='Caption', pos='left') { 
  smmry %>%
    xtable() %>% 
    kable(caption = title, digits = 4) %>%
    kable_styling(position = "center") %>%
    kable_styling(bootstrap_options = "striped", full_width = F,
    position = pos)
}

xkablesummary = function(df) { 
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% 
    apply( 2, function(x) stringr::str_trim(x, "right")) 
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  s %>%
    xkabledply("Table: Statistics summary.", "center")

}

xkablevif = function(model) {
  vifs = table( names(model$coefficients)[2:length(model$coefficients)] ) 
  vifs[] = faraway::vif(model) 

  vifs %>%
    xtable() %>% 
    kable(caption = "VIFs of the model", digits = 4, col.names = 'VIF') %>% 
    kable_styling(position = "center") %>%
    kable_styling(bootstrap_options = "striped", full_width = F,
    position = "left")
}
```

```{r DataSummary_KP, results='asis'}
print('A Summary of the 1960\'s Dataset:')
xkablesummary(data1960_X)
print('A Summary of the 2000\'s Dataset:')
xkablesummary(data2000_X)
```

# Variable Anaylsis and Approach - Rich

The purpose of this study is to identify any link or lackthereof between the popularity of a song and seven other factors, tempo, key, mode, valence, danceability, energy, and liveness, based on track metric data computed from Spotify.  For the two decades under review, 1960 and 2000, this study will quantify the specific differences in musical factors between popular and not-popular songs from each decade as well as between popular songs between the decades via three models: **K-Nearest Neighbor**, **Random Forest**, and **Logistic Regression**.  For each model, the accuracy of the model in predicting the popularity of song from its musical factors will be assessed and the principal musical features from the model with the highest accuracy will be compared between popular and non-popular songs within and between decades.

# Exploratory Data Analysis - Rich

A cursory review of the song feature values for just popular between the two decades, shown below, shows a difference in the mean values for multiple song features, for instance: From the 1960's to the early 2000's, the mean valence noticeably decreases while the mean danceability and energy values noticeably increase.

``` {r PopDataSummary_KP}
# Provide a summary of the columns and data within the data
print('A Summary of the Popular 2000\'s Tracks:')
xkablesummary(subset(data2000, target == 1, select = c('tempo', 'key', 'valence', 'danceability', 'energy', 'liveness', 'mode')))

print('A Summary of the Popular 1960\'s Tracks:')
xkablesummary(subset(data1960, target == 1, select = c('tempo', 'key', 'valence', 'danceability', 'energy', 'liveness', 'mode')))
```

A presentation and analysis of the distributions of each of the variables prior to modeling is presented below.  For each histogram, data from the 1960's will be presented consistently in blue, while data from the 2000's will be presented in red.  Data from popular songs will be displayed in histograms overlaping the collective data with a darker shade of blue or red for 1960 and 2000 data, respectively.  The Shapiro-Wilke Test for determining normality of data has a null hypothesis (H<sub>0</sub>:) stating the data is normally distributed; that is to say, a probablity value below 0.05 (corresponding to a confidence interval of 95%) from the test means that the null hypothesis must be rejected, and the data to which the test was applied is not normally-distributed.

When comparing the statistical equivalency of two values, a Two-Tailed T-Test is performed to determine whether the two values are statistically equivalent.  The null hypothesis for the Two-Tailed T-Test ((H<sub>0</sub>:) is that the two values are statistically equivalent (or their true difference in mean value is equal to 0); that is to say, a probability value less than 0.025 (corresponding to a confidence interval of 95%) from the test means that the null hypothesis must be rejected, and the two values are **not** statistically equivalent.  If the mean values between popular and non-popular songs or popoular songs between decades for a particular feature are **not** statisitically different, it most probably means that the feature is not unique between either classification and will not be considered significant during model processing.

## An Exploration of Tempo

### Tempo Histograms
Discussed above, **Tempo** is the speed or pace of a given song and is often measured in beats per minute.  Distributions of the tempo of `r nrow(data1960)` songs from the 1960's and `r nrow(data2000)` songs from the 2000's are presented below with darker shades showing the distributions of the popular songs within each dataset and the mean value of each set represented by a dashed line:

```{r EDA_Tempo_Gude}
# Set up a conditional for mean lines
temp60Line <- ddply(data1960, "target", summarise, tempo.mean=mean(tempo))
temp00Line <- ddply(data2000, "target", summarise, tempo.mean=mean(tempo))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = data1960, aes(x = tempo, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Tempo in 1960's Songs") +
  labs(x="Tempo (Beats/Minute)", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#00aaaa", "#000080")) +
  scale_fill_manual(values=c("#00aaaa", "#000080")) +
  geom_vline(data= temp60Line, aes(xintercept=tempo.mean, color=target),
             linetype="dashed")

# Produce a two part graph for popular and non-popular songs from 2000
ggplot(data = data2000, aes(x = tempo, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Tempo in 2000's Songs") +
  labs(x="Tempo (Beats/Minute)", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("firebrick2", "red4")) +
  scale_fill_manual(values=c("firebrick2", "red4")) +
  geom_vline(data= temp00Line, aes(xintercept=tempo.mean, color=target),
             linetype="dashed")
```

From the general trends in the histogram data above, it can be seen that the tempo for popular songs in 1960 skews higher than for non-popular songs, from an average tempo of `r mean(subset(data1960, target == 1)$tempo)` beats per minute for popular songs versus `r mean(subset(data1960, target == 0)$tempo)` beats per minute for non-popular songs.  Whereas the reverse is true for popular songs in the 2000's - the tempo for popular songs in 2000 skews lower than for non-popular songs, from an average tempo of `r mean(subset(data2000, target == 1)$tempo)` beats per minute for popular songs versus `r mean(subset(data2000, target == 0)$tempo)` beats per minute for non-popular songs.

The distributions of tempo for just the popular songs from the two decades under review are presented below:

```{r EDA_PopTempo_Gude}
# Set up a conditional for mean lines
tempPopLine <- ddply(dataPopular, "year", summarise, tempo.mean=mean(tempo))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = dataPopular, aes(x = tempo, fill = year)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Tempo in Popular Songs from the 1960's and 2000's") +
  labs(x="Tempo (Beats/Minute)", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#000080", "red4")) +
  scale_fill_manual(values=c("#000080", "red4")) +
  geom_vline(data= tempPopLine, aes(xintercept=tempo.mean, color=year),
             linetype="dashed")
```

From the general trend in the histogram data above, it can be seen that the tempo distribution of popular songs is different between the two decades, with the 1960's having a sharper peak of tempos near the mean for the decade and the 2000's distribution being more skewed left in comparison.  The mean tempo of either decade appears to be distinct as well, but only just so - the two values are very similar. Additional T-Testing may confirm if the two values can be considered statistically equivalent or not.

### Tempo Normality Testing
While the shape of each of the histograms from each decade and popularity potentially appear normally-distributed, the Shapiro-Wilke Test (H<sub>0</sub>: the data is normally-distributed) identifies the following for each sub-category of decade and popularity:

``` {r test_tempo_Gude, include=F}
format(shapiro.test(subset(data1960,target ==1)$tempo)$p.value, digits = 4)
format(shapiro.test(subset(data1960,target ==0)$tempo)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==1)$tempo)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==0)$tempo)$p.value, digits = 4)
```

**Shapiro-Wilke Normality Test:**
1960's Popular Probabality Value:       `r format(shapiro.test(subset(data1960,target ==1)$tempo)$p.value, digits = 4)`
1960's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data1960,target ==0)$tempo)$p.value, digits = 4)`
2000's Popular Probabality Value:       `r format(shapiro.test(subset(data2000,target ==1)$tempo)$p.value, digits = 4)`
2000's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data2000,target ==0)$tempo)$p.value, digits = 4)`

For all of the categories above, the Shapiro-Wilke probability was significantly less than 0.05 and, thus, the data is not normally distributed.

### Tempo Difference Among Popular Songs Between Decades
Considering only popular music, the mean tempo for 1960's music is `r format(mean(subset(data1960, target == 1)$tempo), digits = 1)` beats per minute, and the mean tempo for 2000's music is `r format(mean(subset(data2000, target == 1)$tempo), digits = 1)` beats per minute.  A Two-Tailed T-Test (H<sub>0</sub>: the two means are statistically equivalent or their true difference in mean value is equal to 0) is performed to determine whether the two mean tempos from either decade are statistically equivalent:

```{r tempo_TTest_Gude}
# Perform tst
ttest_tempo = t.test(subset(data1960,target ==1)$tempo, subset(data2000,target ==1)$tempo)
ttest_tempo
```

With a probability value of `r ttest_tempo$p.value` (p > 0.025), the T-Test null hypothesis is **not** rejected (albeit just barely), and the mean tempo values between the 1960's and 2000's decades should be considered equivalent (or their true difference in means is equal to 0).

From the additional T-Testing below, the mean tempo values between popular and non-popular songs from each decade should **not** be considered equivalent (or their true difference in means is not equal to 0)

```{r tempo_TTest_Gude2}
# Perform tst
ttest_tempo60 = t.test(subset(data1960,target ==0)$tempo, subset(data1960,target ==1)$tempo)
ttest_tempo60

ttest_tempo00 = t.test(subset(data2000,target ==0)$tempo, subset(data2000,target ==1)$tempo)
ttest_tempo00
```

The conclusion of the exploratory data analysis of song tempo in the two datasets from the 1960's and the 2000's is no collection of popular or non-popular songs were normally-distributed, and the mean values in tempo were distinct (non-equivalent) between popular and non-popular songs in each decade, but the mean values between popular songs from each decade were not distinct.

## An Exploration of Key

### Key Histograms
Discussed above, the **Key** of a song is the average pitch of a song, or the pitch around which a song fluctuates.  There are 12 keys represented in the integer mapping from both the 1960's and 2000's data; specifically, the following intergers map to the accompanying keys: 0 = C key, 1 = C*#*/D*b*, 2 = D, 3 = D*#*/E*b*, 4 = E, 5 = F, 6 = F*#*/G*b*, 7 = G, 8 = G*#*/A*b*, 9 = A, 10 = A*#*/B*b*, and 11 = B.  Distributions of the keys of `r nrow(data1960)` songs from the 1960's and `r nrow(data2000)` songs from the 2000's are presented below with darker shades showing the distributions of the popular songs within each dataset and the mean value of each set represented by a dashed line:

```{r EDA_Key_Gude}
# Set up a conditional for mean lines
key60Line <- ddply(data1960, "target", summarise, key.mean=mean(key))
key00Line <- ddply(data2000, "target", summarise, key.mean=mean(key))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = data1960, aes(x = key, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Key in 1960's Songs") +
  labs(x="Key", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#00aaaa", "#000080")) +
  scale_fill_manual(values=c("#00aaaa", "#000080")) +
  geom_vline(data= key60Line, aes(xintercept=key.mean, color=target),
             linetype="dashed")

# Produce a two part graph for popular and non-popular songs from 2000
ggplot(data = data2000, aes(x = key, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Key in 2000's Songs") +
  labs(x="Key", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("firebrick2", "red4")) +
  scale_fill_manual(values=c("firebrick2", "red4")) +
  geom_vline(data= key00Line, aes(xintercept=key.mean, color=target),
             linetype="dashed")
```

From the general trends in the histogram data above, there may be a slight higher shift in the distribution of keys for popular songs in 1960 versus the keys of non-popular songs, from an average key of `r mean(subset(data1960, target == 1)$key)` for popular songs versus `r mean(subset(data1960, target == 0)$key)` for non-popular songs.  Whereas the reverse may be true for popular songs in the 2000's - the key for popular songs in 2000 skews just slightly lower than for non-popular songs, from an average key of `r mean(subset(data2000, target == 1)$key)` for popular songs versus `r mean(subset(data2000, target == 0)$key)` for non-popular songs, although the difference appearance to be negligible.

The distributions of key for just the popular songs from the two decades under review are presented below:

```{r EDA_PopKey_Gude}
# Set up a conditional for mean lines
keyPopLine <- ddply(dataPopular, "year", summarise, key.mean=mean(key))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = dataPopular, aes(x = key, fill = year)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Key in Popular Songs from the 1960's and 2000's") +
  labs(x="Key", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#000080", "red4")) +
  scale_fill_manual(values=c("#000080", "red4")) +
  geom_vline(data= keyPopLine, aes(xintercept=key.mean, color=year),
             linetype="dashed")
```

From the general trend in the histogram data above, it can be seen that the tempo distribution of popular songs is different between the two decades, with the 1960's trending to the lower key values in comparison to the 2000's distribution.  The mean key of either decade appears to be distinct as well with the 1960's having a lower average key in comparison to the 2000's. Additional T-Testing may confirm if the two values can be considered statistically equivalent or not.

### Key Normality Testing
The shape of each of the histograms from each decade and popularity do not appear normally-distributed; instead they appear closer to a uniform distribution.  To confirm this observation, the Shapiro-Wilke Test (H<sub>0</sub>: the data is normally-distributed) identifies the following for each sub-category of decade and popularity for key distributions:

``` {r test_key_Gude, include=F}
format(shapiro.test(subset(data1960,target ==1)$key)$p.value, digits = 4)
format(shapiro.test(subset(data1960,target ==0)$key)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==1)$key)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==0)$key)$p.value, digits = 4)
```

**Shapiro-Wilke Normality Test:**
1960's Popular Probabality Value:       `r format(shapiro.test(subset(data1960,target ==1)$key)$p.value, digits = 4)`
1960's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data1960,target ==0)$key)$p.value, digits = 4)`
2000's Popular Probabality Value:       `r format(shapiro.test(subset(data2000,target ==1)$key)$p.value, digits = 4)`
2000's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data2000,target ==0)$key)$p.value, digits = 4)`

For all of the categories above, the Shapiro-Wilke probability was significantly less than 0.05 and, thus, none of the key distributions should be considered normally-distributed.

### Key Difference Among Popular Songs Between Decades
Considering only popular music, the mean key for 1960's music is `r format(mean(subset(data1960, target == 1)$key), digits = 1)`, and the mean key for 2000's music is `r format(mean(subset(data2000, target == 1)$key), digits = 1)`.  A Two-Tailed T-Test (H<sub>0</sub>: the two means are statistically equivalent or their true difference in mean value is equal to 0) is performed to determine whether the two mean keys from either decade are statistically equivalent:

```{r key_TTest_Gude}
# Perform t-test
ttest_key = t.test(subset(data1960,target ==1)$key, subset(data2000,target ==1)$key)
ttest_key
```

With a probability value of `r ttest_key$p.value` (p > 0.025), the T-Test null hypothesis is **not** rejected, and the mean key values between the 1960's and 2000's decades should be considered equivalent (or their true difference in means is equal to 0).

From the additional T-Testing below, the mean key values between popular and non-popular songs from each decade should **not** be considered equivalent (or their true difference in means is not equal to 0)

```{r key_TTest_Gude2}
# Perform tst
ttest_key60 = t.test(subset(data1960,target ==0)$key, subset(data1960,target ==1)$key)
ttest_key60

ttest_key00 = t.test(subset(data2000,target ==0)$key, subset(data2000,target ==1)$key)
ttest_key00
```

The conclusion of the exploratory data analysis of song key in the two datasets from the 1960's and the 2000's is no collection of popular or non-popular songs were normally-distributed, and the mean values in key were not distinct between popular and non-popular songs in each decade and between popular songs from each decade.

## An Exploration of Mode

### Mode Bar Charts
Discussed above, the **Mode** of a song is the interval at which pitches are expressed in a song and is expressed as either "minor" or "major", where the major key is associated with melodies that are cheery or bright while the minor key is associated with melancholic melodies.  The mode is a binary categorical variable with an integer mapping of 0 for minor keys and 1 for major keys.  Bar charts of the mode values of `r nrow(data1960)` songs from the 1960's and `r nrow(data2000)` songs from the 2000's are presented below with darker shades showing the mode values of the popular songs within each dataset (bars are not overlapping):

```{r EDA_Mode_Gude}
# Produce a two part bar chart for popular and non-popular songs from 1960
ggplot(data = data1960, aes(mode)) + 
  geom_bar(aes(fill = target)) +
  labs(title="Bar Chart of Mode in 1960's Songs") +
  labs(x="Mode", y="Count") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_fill_manual(values=c("#00aaaa", "#000080")) +
  theme_minimal()

# Produce a two part bar chart for popular and non-popular songs from 1960
ggplot(data = data2000, aes(mode)) + 
  geom_bar(aes(fill = target)) +
  labs(title="Bar Chart of Mode in 2000's Songs") +
  labs(x="Mode", y="Count") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_fill_manual(values=c("firebrick2", "red4")) +
  theme_minimal()
```

The percentage of popular songs from the 1960's with a major key (mode = 1) is `r mean(subset(data1960, target==1)$mode)*100`%, which is greater than the percentage of non-popular songs from the 1960's with a major key at `r mean(subset(data1960, target==0)$mode)*100`%.  Similarly, the percentage of popular songs from the 2000's with a major key (`r mean(subset(data2000, target==1)$mode)*100`%) is greater than the percentage of non-popular songs with a major key (`r mean(subset(data2000, target==0)$mode)*100`%).  These trends show a general shift in popular songs towards the major key which is associated with happy or bright melodies compared to non-popular songs.

```{r Gude_Roughcalcs, include = F}
mean(subset(data1960, target==1)$mode)*100
mean(subset(data1960, target==0)$mode)*100
mean(subset(data2000, target==1)$mode)*100
mean(subset(data2000, target==0)$mode)*100
nrow(subset(data1960, mode==1))/nrow(data1960)
```

A bar chart of the mode values for just the popular songs from the two decades under review is presented below:

```{r EDA_PopMode_Gude}
# Produce a two part bar chart for popular songs from both decades
ggplot(data = dataPopular, aes(mode)) + 
  geom_bar(aes(fill = year)) +
  labs(title="Bar Chart of Mode in Popular Songs from the 1960's and 2000's") +
  labs(x="Mode", y="Count") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_fill_manual(values=c("#000080", "red4")) +
  theme_minimal()
```

The percentage of songs that are in the major key from popular 1960's songs (`r mean(subset(data1960, target==1)$mode)*100`%) appears to be significantly higher than the percentage of songs that are in the major key from popular 2000's songs (`r mean(subset(data2000, target==1)$mode)*100`%).  Additional T-Testing may confirm if the two values can be considered statistically different or not.

### Mode Difference Among Popular Songs Between Decades
Considering only popular music, the mean mode (equivalent to the percentage of songs with a major key mode) for 1960's music is `r mean(subset(data1960, target==1)$mode)`, and the mean key for 2000's music is `r mean(subset(data2000, target==1)$mode)`.  A Two-Tailed T-Test (H<sub>0</sub>: the two means are statistically equivalent or their true difference in mean value is equal to 0) is performed to determine whether the two mean keys from either decade are statistically equivalent:

```{r mode_TTest_Gude}
# Perform t-test
ttest_mode = t.test((subset(data1960, target==1)$mode), (subset(data2000, target==1)$mode))
ttest_mode
```

With a probability value of `r ttest_mode$p.value` (p < 0.025), the T-Test null hypothesis is rejected, and the mean mode values between the 1960's and 2000's decades should **not** be considered equivalent (or their true difference in means is not equal to 0).  The 1960's had a statistically significant greater proportion of popular songs in the major key compared to popular songs in the 2000's.

From the additional T-Testing below, the mean mode values between popular and non-popular songs from each decade should **not** be considered equivalent (or their true difference in means is not equal to 0)

```{r mode_TTest_Gude2}
# Perform tst
ttest_mode60 = t.test(subset(data1960,target ==0)$mode, (subset(data1960,target ==1)$mode))
ttest_mode60

ttest_mode00 = t.test(subset(data2000,target ==0)$mode, (subset(data2000,target ==1)$mode))
ttest_mode00
```

The conclusion of the exploratory data analysis of song mode in the two datasets from the 1960's and the 2000's is the mean values in mode were distinct (non-equivalent) between popular and non-popular songs in each decade and between popular songs from each decade.

## An Exploration of Valence

### Valence Histograms
Discussed above, the **Valence** describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).  Valence is a numerical value between 0 and 1, inclusive.  Distributions of the valence values of `r nrow(data1960)` songs from the 1960's and `r nrow(data2000)` songs from the 2000's are presented below with darker shades showing the distributions of the popular songs within each dataset and the mean value of each set represented by a dashed line:

```{r EDA_Valence_Gude}
# Set up a conditional for mean lines
val60Line <- ddply(data1960, "target", summarise, val.mean=mean(valence))
val00Line <- ddply(data2000, "target", summarise, val.mean=mean(valence))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = data1960, aes(x = valence, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Valence in 1960's Songs") +
  labs(x="Valence", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#00aaaa", "#000080")) +
  scale_fill_manual(values=c("#00aaaa", "#000080")) +
  geom_vline(data= val60Line, aes(xintercept=val.mean, color=target),
             linetype="dashed")

# Produce a two part graph for popular and non-popular songs from 2000
ggplot(data = data2000, aes(x = valence, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Valence in 2000's Songs") +
  labs(x="Valence", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("firebrick2", "red4")) +
  scale_fill_manual(values=c("firebrick2", "red4")) +
  geom_vline(data= val00Line, aes(xintercept=val.mean, color=target),
             linetype="dashed")
```

From the general trends in the histogram data above, there is a clear right shift in the distribution of valence values (towards more positive and uplifting qualities) for popular songs in the 1960's and 2000's compared to the valence values of non-popular songs, from an average valence of `r mean(subset(data1960, target == 1)$valence)` and `r mean(subset(data2000, target == 1)$valence)` for popular songs in the 1960's and 2000's, respectively, versus `r mean(subset(data1960, target == 0)$valence)` and `r mean(subset(data2000, target == 0)$valence)` for non-popular songs in the 1960's and 2000's, respectively.  In addition, none of the distributions of valence values appear to be normally-distributed.

The distributions of valence values for just the popular songs from the two decades under review are presented below:

```{r EDA_PopValence_Gude}
# Set up a conditional for mean lines
valPopLine <- ddply(dataPopular, "year", summarise, val.mean=mean(valence))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = dataPopular, aes(x = valence, fill = year)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Valence in Popular Songs from the 1960's and 2000's") +
  labs(x="Valence", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#000080", "red4")) +
  scale_fill_manual(values=c("#000080", "red4")) +
  geom_vline(data= valPopLine, aes(xintercept=val.mean, color=year),
             linetype="dashed")
```

From the general trend in the histogram data above, it can be seen that the valence distribution of popular songs is different between the two decades, with the 1960's trending to higher valence values in comparison to the 2000's distribution.  The mean valence of either decade appears to be distinct as well with the 1960's having a much higher, average valence value in comparison to the 2000's. Additional T-Testing may confirm if the two values can be considered statistically different or not.

### Valence Normality Testing
The shape of each of the histograms from each decade and popularity do not appear normally-distributed; instead they appear closer to a uniform distribution or triangular in the case of popular 1960's songs and non-popular 2000's songs.  To confirm this observation, the Shapiro-Wilke Test (H<sub>0</sub>: the data is normally-distributed) identifies the following for each sub-category of decade and popularity for valence distributions:

``` {r test_valence_Gude, include=F}
format(shapiro.test(subset(data1960,target ==1)$valence)$p.value, digits = 4)
format(shapiro.test(subset(data1960,target ==0)$valence)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==1)$valence)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==0)$valence)$p.value, digits = 4)
```

**Shapiro-Wilke Normality Test:**
1960's Popular Probabality Value:       `r format(shapiro.test(subset(data1960,target ==1)$valence)$p.value, digits = 4)`
1960's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data1960,target ==0)$valence)$p.value, digits = 4)`
2000's Popular Probabality Value:       `r format(shapiro.test(subset(data2000,target ==1)$valence)$p.value, digits = 4)`
2000's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data2000,target ==0)$valence)$p.value, digits = 4)`

For all of the categories above, the Shapiro-Wilke probability was significantly less than 0.05 and, thus, none of the valence distributions should be considered normally-distributed.

### Valence Difference Among Popular Songs Between Decades
Considering only popular music, the mean valence for 1960's music is `r format(mean(subset(data1960, target == 1)$valence), digits = 3)`, and the mean valence for 2000's music is `r format(mean(subset(data2000, target == 1)$valence), digits = 3)`.  A Two-Tailed T-Test (H<sub>0</sub>: the two means are statistically equivalent or their true difference in mean value is equal to 0) is performed to determine whether the two mean valence values from either decade are statistically equivalent:

```{r valence_TTest_Gude}
# Perform t-test
ttest_val = t.test(subset(data1960,target ==1)$valence, subset(data2000,target ==1)$valence)
ttest_val
```

With a probability value of `r ttest_val$p.value` (p < 0.025), the T-Test null hypothesis is rejected, and the mean valence values between the 1960's and 2000's decades should **not** be considered equivalent (or their true difference in means is not equal to 0).  The 1960's had a statistically-significant greater average valence value among popular songs compared to popular songs in the 2000's.

From the additional T-Testing below, the mean valence values between popular and non-popular songs from each decade should **not** be considered equivalent (or their true difference in means is not equal to 0)

```{r valence_TTest_Gude2}
# Perform tst
ttest_valence60 = t.test(subset(data1960,target ==0)$valence, (subset(data1960,target ==1)$valence))
ttest_valence60

ttest_valence00 = t.test(subset(data2000,target ==0)$valence, (subset(data2000,target ==1)$valence))
ttest_valence00
```

The conclusion of the exploratory data analysis of song valence in the two datasets from the 1960's and the 2000's is no collection of popular or non-popular songs were normally-distributed, and the mean values in valence were distinct (non-equivalent) between popular and non-popular songs in each decade and between popular songs from each decade.

## An Exploration of Danceability

### Danceability Histograms
Discussed above, the **Danceability** describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.  Danceability is a numerical value between 0 and 1, inclusive.  Distributions of the danceability values of `r nrow(data1960)` songs from the 1960's and `r nrow(data2000)` songs from the 2000's are presented below with darker shades showing the distributions of the popular songs within each dataset and the mean value of each set represented by a dashed line:

```{r EDA_Danceability_Gude}
# Set up a conditional for mean lines
danc60Line <- ddply(data1960, "target", summarise, danc.mean=mean(danceability))
danc00Line <- ddply(data2000, "target", summarise, danc.mean=mean(danceability))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = data1960, aes(x = danceability, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Danceability in 1960's Songs") +
  labs(x="Danceability", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#00aaaa", "#000080")) +
  scale_fill_manual(values=c("#00aaaa", "#000080")) +
  geom_vline(data= danc60Line, aes(xintercept=danc.mean, color=target),
             linetype="dashed")

# Produce a two part graph for popular and non-popular songs from 2000
ggplot(data = data2000, aes(x = danceability, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Danceability in 2000's Songs") +
  labs(x="Danceability", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("firebrick2", "red4")) +
  scale_fill_manual(values=c("firebrick2", "red4")) +
  geom_vline(data= danc00Line, aes(xintercept=danc.mean, color=target),
             linetype="dashed")
```

From the general trends in the histogram data above, there is a clear right shift in the distribution of danceability values (towards more positive and uplifting qualities) for popular songs in the 1960's and 2000's compared to the danceability values of non-popular songs, from an average danceability of `r mean(subset(data1960, target == 1)$danceability)` and `r mean(subset(data2000, target == 1)$danceability)` for popular songs in the 1960's and 2000's, respectively, versus `r mean(subset(data1960, target == 0)$danceability)` and `r mean(subset(data2000, target == 0)$danceability)` for non-popular songs in the 1960's and 2000's, respectively.  In addition, all of the distributions of danceability values appear to be normally-distributed.

The distributions of danceability values for just the popular songs from the two decades under review are presented below:

```{r EDA_PopDanceability_Gude}
# Set up a conditional for mean lines
dancPopLine <- ddply(dataPopular, "year", summarise, danc.mean=mean(danceability))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = dataPopular, aes(x = danceability, fill = year)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Danceability in Popular Songs from the 1960's and 2000's") +
  labs(x="Danceability", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#000080", "red4")) +
  scale_fill_manual(values=c("#000080", "red4")) +
  geom_vline(data= dancPopLine, aes(xintercept=danc.mean, color=year),
             linetype="dashed")
```

From the general trend in the histogram data above, it can be seen that the danceability distribution of popular songs is different between the two decades, with the 2000's trending to higher danceability values in comparison to the 1960's distribution.  The mean danceability of either decade appears to be distinct as well with the 2000's having a much higher, average danceability value in comparison to the 1960's. Additional T-Testing may confirm if the two values can be considered statistically different or not.

### Danceability Normality Testing
The shape of each of the histograms from each decade and popularity appear to be normally-distributed.  To confirm this observation, the Shapiro-Wilke Test (H<sub>0</sub>: the data is normally-distributed) identifies the following for each sub-category of decade and popularity for danceability distributions:

``` {r test_danceability_Gude, include=F}
format(shapiro.test(subset(data1960,target ==1)$danceability)$p.value, digits = 4)
format(shapiro.test(subset(data1960,target ==0)$danceability)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==1)$danceability)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==0)$danceability)$p.value, digits = 4)
```

**Shapiro-Wilke Normality Test:**
1960's Popular Probabality Value:       `r format(shapiro.test(subset(data1960,target ==1)$danceability)$p.value, digits = 4)`
1960's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data1960,target ==0)$danceability)$p.value, digits = 4)`
2000's Popular Probabality Value:       `r format(shapiro.test(subset(data2000,target ==1)$danceability)$p.value, digits = 4)`
2000's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data2000,target ==0)$danceability)$p.value, digits = 4)`

For all of the categories above, the Shapiro-Wilke probability was significantly less than 0.05 and, thus in contrast to potential appearances, none of the danceability distributions should be considered normally-distributed.

### Danceability Difference Among Popular Songs Between Decades
Considering only popular music, the mean danceability for 1960's music is `r format(mean(subset(data1960, target == 1)$danceability), digits = 3)`, and the mean danceability for 2000's music is `r format(mean(subset(data2000, target == 1)$danceability), digits = 3)`.  A Two-Tailed T-Test (H<sub>0</sub>: the two means are statistically equivalent or their true difference in mean value is equal to 0) is performed to determine whether the two mean danceability values from either decade are statistically equivalent:

```{r danceability_TTest_Gude}
# Perform t-test
ttest_danc = t.test(subset(data1960,target ==1)$danceability, subset(data2000,target ==1)$danceability)
ttest_danc
```

With a probability value of `r ttest_danc$p.value` (p < 0.025), the T-Test null hypothesis is rejected, and the mean danceability values between the 1960's and 2000's decades should **not** be considered equivalent (or their true difference in means is not equal to 0).  The 2000's had a statistically-significant greater average danceability value among popular songs compared to popular songs in the 1960's.

From the additional T-Testing below, the mean danceability values between popular and non-popular songs from each decade should **not** be considered equivalent (or their true difference in means is not equal to 0)

```{r danceability_TTest_Gude2}
# Perform tst
ttest_danceability60 = t.test(subset(data1960,target ==0)$danceability, (subset(data1960,target ==1)$danceability))
ttest_danceability60

ttest_danceability00 = t.test(subset(data2000,target ==0)$danceability, (subset(data2000,target ==1)$danceability))
ttest_danceability00
```

The conclusion of the exploratory data analysis of song danceability in the two datasets from the 1960's and the 2000's is no collection of popular or non-popular songs were normally-distributed, and the mean values in danceability were distinct (non-equivalent) between popular and non-popular songs in each decade and between popular songs from each decade.

## An Exploration of Energy

### Energy Histograms
Discussed above, **Energy** represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.  Danceability is a numerical value between 0 and 1, inclusive.  Distributions of the energy values of `r nrow(data1960)` songs from the 1960's and `r nrow(data2000)` songs from the 2000's are presented below with darker shades showing the distributions of the popular songs within each dataset and the mean value of each set represented by a dashed line:

```{r EDA_Energy_Gude}
# Set up a conditional for mean lines
energy60Line <- ddply(data1960, "target", summarise, energy.mean=mean(energy))
energy00Line <- ddply(data2000, "target", summarise, energy.mean=mean(energy))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = data1960, aes(x = energy, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Energy in 1960's Songs") +
  labs(x="Energy", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#00aaaa", "#000080")) +
  scale_fill_manual(values=c("#00aaaa", "#000080")) +
  geom_vline(data= energy60Line, aes(xintercept=energy.mean, color=target),
             linetype="dashed")

# Produce a two part graph for popular and non-popular songs from 2000
ggplot(data = data2000, aes(x = energy, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Energy in 2000's Songs") +
  labs(x="Energy", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("firebrick2", "red4")) +
  scale_fill_manual(values=c("firebrick2", "red4")) +
  geom_vline(data= energy00Line, aes(xintercept=energy.mean, color=target),
             linetype="dashed")
```

From the general trends in the histogram data above, there is a clear right shift in the distribution of energy values for popular songs in the 1960's and 2000's compared to the energy values of non-popular songs, from an average energy of `r mean(subset(data1960, target == 1)$energy)` and `r mean(subset(data2000, target == 1)$energy)` for popular songs in the 1960's and 2000's, respectively, versus `r mean(subset(data1960, target == 0)$energy)` and `r mean(subset(data2000, target == 0)$energy)` for non-popular songs in the 1960's and 2000's, respectively.  In addition, none of the distributions of energy values appear to be normally-distributed, with significant skews in the distribution either to the left or right in each of the distributions except popular 1960's songs which still has two many energy values near the tails of the distribution to be considered normally-distributed.

The distributions of energy values for just the popular songs from the two decades under review are presented below:

```{r EDA_PopEnergy_Gude}
# Set up a conditional for mean lines
energyPopLine <- ddply(dataPopular, "year", summarise, energy.mean=mean(energy))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = dataPopular, aes(x = energy, fill = year)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Energy in Popular Songs from the 1960's and 2000's") +
  labs(x="Energy", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#000080", "red4")) +
  scale_fill_manual(values=c("#000080", "red4")) +
  geom_vline(data= energyPopLine, aes(xintercept=energy.mean, color=year),
             linetype="dashed")
```

From the general trend in the histogram data above, it can be seen that the energy distribution of popular songs is different between the two decades, with the 2000's trending to higher energy values in comparison to the 1960's distribution.  The mean energy of either decade appears to be distinct as well with the 2000's having a much higher, average energy value in comparison to the 1960's. Additional T-Testing may confirm if the two values can be considered statistically different or not.

### Energy Normality Testing
The shape of each of the histograms from each decade and popularity do not appear to be normally-distributed.  To confirm this observation, the Shapiro-Wilke Test (H<sub>0</sub>: the data is normally-distributed) identifies the following for each sub-category of decade and popularity for energy distributions:

``` {r test_energy_Gude, include=F}
format(shapiro.test(subset(data1960,target ==1)$energy)$p.value, digits = 4)
format(shapiro.test(subset(data1960,target ==0)$energy)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==1)$energy)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==0)$energy)$p.value, digits = 4)
```

**Shapiro-Wilke Normality Test:**
1960's Popular Probabality Value:       `r format(shapiro.test(subset(data1960,target ==1)$energy)$p.value, digits = 4)`
1960's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data1960,target ==0)$energy)$p.value, digits = 4)`
2000's Popular Probabality Value:       `r format(shapiro.test(subset(data2000,target ==1)$energy)$p.value, digits = 4)`
2000's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data2000,target ==0)$energy)$p.value, digits = 4)`

For all of the categories above, the Shapiro-Wilke probability was significantly less than 0.05 and, thus, none of the energy distributions should be considered normally-distributed.

### Energy Difference Among Popular Songs Between Decades
Considering only popular music, the mean energy for 1960's music is `r format(mean(subset(data1960, target == 1)$energy), digits = 3)`, and the mean energy for 2000's music is `r format(mean(subset(data2000, target == 1)$energy), digits = 3)`.  A Two-Tailed T-Test (H<sub>0</sub>: the two means are statistically equivalent or their true difference in mean value is equal to 0) is performed to determine whether the two mean energy values from either decade are statistically equivalent:

```{r energy_TTest_Gude}
# Perform t-test
ttest_energy = t.test(subset(data1960,target ==1)$energy, subset(data2000,target ==1)$energy)
ttest_energy
```

With a probability value of `r ttest_energy$p.value` (p < 0.025), the T-Test null hypothesis is rejected, and the mean energy values between the 1960's and 2000's decades should **not** be considered equivalent (or their true difference in means is not equal to 0).  The 2000's had a statistically-significant greater average energy value among popular songs compared to popular songs in the 1960's.

From the additional T-Testing below, the mean energy values between popular and non-popular songs from each decade should **not** be considered equivalent (or their true difference in means is not equal to 0)

```{r energy_TTest_Gude2}
# Perform tst
ttest_energy60 = t.test(subset(data1960,target ==0)$energy, (subset(data1960,target ==1)$energy))
ttest_energy60

ttest_energy00 = t.test(subset(data2000,target ==0)$energy, (subset(data2000,target ==1)$energy))
ttest_energy00
```

The conclusion of the exploratory data analysis of song energy in the two datasets from the 1960's and the 2000's is no collection of popular or non-popular songs were normally-distributed, and the mean values in energy were distinct (non-equivalent) between popular and non-popular songs in each decade and between popular songs from each decade.

## An Exploration of Liveness

### Liveness Histograms
Discussed above, **Liveness** detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.  Liveness is a numerical value between 0 and 1, inclusive.  Distributions of the liveness values of `r nrow(data1960)` songs from the 1960's and `r nrow(data2000)` songs from the 2000's are presented below with darker shades showing the distributions of the popular songs within each dataset and the mean value of each set represented by a dashed line:

```{r EDA_Liveness_Gude}
# Set up a conditional for mean lines
live60Line <- ddply(data1960, "target", summarise, live.mean=mean(liveness))
live00Line <- ddply(data2000, "target", summarise, live.mean=mean(liveness))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = data1960, aes(x = liveness, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Liveness in 1960's Songs") +
  labs(x="Liveness", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#00aaaa", "#000080")) +
  scale_fill_manual(values=c("#00aaaa", "#000080")) +
  geom_vline(data= live60Line, aes(xintercept=live.mean, color=target),
             linetype="dashed")

# Produce a two part graph for popular and non-popular songs from 2000
ggplot(data = data2000, aes(x = liveness, fill = target)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Liveness in 2000's Songs") +
  labs(x="Liveness", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("firebrick2", "red4")) +
  scale_fill_manual(values=c("firebrick2", "red4")) +
  geom_vline(data= live00Line, aes(xintercept=live.mean, color=target),
             linetype="dashed")
```

From the general trends in the histogram data above, there is a clear and pronounced left shift in all the distributions of liveness values, potentially identifying a general preference for studio-produced music qualities in producers or listeners.  It can be seen that the liveness for popular songs in 1960 skews higher than for non-popular songs, from an average liveness of `r mean(subset(data1960, target == 1)$liveness)` for popular songs versus `r mean(subset(data1960, target == 0)$liveness)` for non-popular songs.  Whereas the reverse is true for popular songs in the 2000's - the liveness for popular songs in 2000 skews lower than for non-popular songs, from an average liveness of `r mean(subset(data2000, target == 1)$liveness)` for popular songs versus `r mean(subset(data2000, target == 0)$liveness)` for non-popular songs.

The distributions of liveness values for just the popular songs from the two decades under review are presented below:

```{r EDA_PopLiveness_Gude}
# Set up a conditional for mean lines
livePopLine <- ddply(dataPopular, "year", summarise, live.mean=mean(liveness))

# Produce a two part graph for popular and non-popular songs from 1960
ggplot(data = dataPopular, aes(x = liveness, fill = year)) + 
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(title="Histogram for Liveness in Popular Songs from the 1960's and 2000's") +
  labs(x="Liveness", y="Frequency") + 
  theme(legend.position=c(0.9, 0.8)) +
  scale_color_manual(values=c("#000080", "red4")) +
  scale_fill_manual(values=c("#000080", "red4")) +
  geom_vline(data= livePopLine, aes(xintercept=live.mean, color=year),
             linetype="dashed")
```

From the general trend in the histogram data above, it can be seen that the liveness distribution of popular songs is different between the two decades, with the 1960's trending to higher liveness values in comparison to the 2000's distribution.  The mean liveness of either decade appears to be distinct as well with the 1960's having a higher average liveness value in comparison to the 2000's. Additional T-Testing may confirm if the two values can be considered statistically different or not.

### Liveness Normality Testing
The shape of each of the histograms from each decade and popularity do not appear to be normally-distributed.  To confirm this observation, the Shapiro-Wilke Test (H<sub>0</sub>: the data is normally-distributed) identifies the following for each sub-category of decade and popularity for liveness distributions:

``` {r test_liveness_Gude, include=F}
format(shapiro.test(subset(data1960,target ==1)$liveness)$p.value, digits = 4)
format(shapiro.test(subset(data1960,target ==0)$liveness)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==1)$liveness)$p.value, digits = 4)
format(shapiro.test(subset(data2000,target ==0)$liveness)$p.value, digits = 4)
```

**Shapiro-Wilke Normality Test:**
1960's Popular Probabality Value:       `r format(shapiro.test(subset(data1960,target ==1)$liveness)$p.value, digits = 4)`
1960's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data1960,target ==0)$liveness)$p.value, digits = 4)`
2000's Popular Probabality Value:       `r format(shapiro.test(subset(data2000,target ==1)$liveness)$p.value, digits = 4)`
2000's Non-Popular Probabality Value:   `r format(shapiro.test(subset(data2000,target ==0)$liveness)$p.value, digits = 4)`

For all of the categories above, the Shapiro-Wilke probability was significantly less than 0.05 and, thus, none of the liveness distributions should be considered normally-distributed.

### Liveness Difference Among Popular Songs Between Decades
Considering only popular music, the mean liveness for 1960's music is `r format(mean(subset(data1960, target == 1)$liveness), digits = 3)`, and the mean liveness for 2000's music is `r format(mean(subset(data2000, target == 1)$liveness), digits = 3)`.  A Two-Tailed T-Test (H<sub>0</sub>: the two means are statistically equivalent or their true difference in mean value is equal to 0) is performed to determine whether the two mean liveness values from either decade are statistically equivalent:

```{r liveness_TTest_Gude}
# Perform t-test
ttest_liveness = t.test(subset(data1960,target ==1)$liveness, subset(data2000,target ==1)$liveness)
ttest_liveness
```

With a probability value of `r ttest_liveness$p.value` (p < 0.025), the T-Test null hypothesis is rejected, and the mean liveness values between the 1960's and 2000's decades should **not** be considered equivalent (or their true difference in means is not equal to 0).  The 1960's had a statistically-significant greater average liveness value among popular songs compared to popular songs in the 2000's.

From the additional T-Testing below, the mean liveness values between popular and non-popular songs from each decade should **not** be considered equivalent (or their true difference in means is not equal to 0)

```{r liveness_TTest_Gude2}
# Perform tst
ttest_liveness60 = t.test(subset(data1960,target ==0)$liveness, (subset(data1960,target ==1)$liveness))
ttest_liveness60

ttest_liveness00 = t.test(subset(data2000,target ==0)$liveness, (subset(data2000,target ==1)$liveness))
ttest_liveness00
```

The conclusion of the exploratory data analysis of song liveness in the two datasets from the 1960's and the 2000's is no collection of popular or non-popular songs were normally-distributed, and the mean values in liveness were distinct (non-equivalent) between popular and non-popular songs in each decade and between popular songs from each decade.

## EDA Conclusions

The results of the exploratory data analysis above demonstrate that all of the features within this dataset except one have differing mean values between both the popular and non-popular songs from each decade as well as between popular songs from the 1960's and 2000's, and no features had a normal distribution.  'Key' is the only musical attribute that does not have a distinct, statistical difference between mean values or distributions between popular and non-popular songs from each decade as well as popular songs from the 1960's or 2000's.  For this reason, it is not likely that 'Key' will be a significant feature in any model, as it is not likely to add any information gain to the models and distinguish between the target variable.

# Model Exploration

The purpose of this study it to identify: Does the key, valence, tempo, or other musical attributes change between popular American songs, and not popular songs for contrast, from the 1960's versus songs from the early 2000's?  In order to answer this question, this study will utilize three predictive models, K-Nearest Neighbor, Decision Tree, and Logistic Regression, to predict the binary "popularity" feature, popular or not popular.  If a model has greater than 50% accuracy in predicting a binary variable, then the model is considered significant (better than random guessing which has an accuracy of 50%).  The musical attribute features utilized within significant models are those that have the most significant impact.  Accordingly, this study will answer the purpose question by identifying whether each predicitive model is significant and comparing and contrasting the significant musical attributes that appear in the models between popular songs in the 1960s and 2000s.

# K-Nearest Neighbor - Juhne

The project goal is to explore which features determine whether a song is popular or not. K-Nearest Neighbor is a good option to apply on this dataset, because both of our 1960s and 2000s datasets have a lot of numerical features and the target we want to predict on is a catagorical variable which is a perfect situation to apply K-Nearest Neighbor algorithm. Since K-Nearest Neighbor algorithm use distance function to measure distance among each data point, make sure that every predictor has same unit is very important. 

From the previous EDA section it showed that **tempo, key and mode** have much greater mean value compare to other predictors. Therefore, in order to make the most accuary K-Nearest Neighbor model, scaling the data using Z-score before building K-Nearest Neighbor model is very necessary.

On the other hand, since the K-Nearest Neighbor algorithm highly depend on distance function. Cleaning the categorical features like **track and artist** is also very important.
 
From the scaled dataset, notice that **tempo** for 2000s data is smaller than 1960s data in unit factor, the **mode and valence** value range for 2000s data are more spread than 1960 data showing that musice in 2000 have more varieties than music in 1960. However, the 2000s' musics are in general have more **energy** than 1960's musics. 

## K-Nearest Neighbor Scaling

### scale (data 2000)

Here we apply simple standard scaling technic which is z-score scaling, that scale our numerical variables with their standard deviation so that all of the scaled numerical variables will have their mean equal to zero. This approach is suitable in most of the time when trying to build model that applies distance function like **Euclidean distance** or **Manhattan distance**.

```{r scale-data-junhe-20}
# scale tempo ~ liveness
df.scaled.20 <- as.data.frame(scale(data2000[3:9], center = TRUE, scale = TRUE))

df.scaled.20$target <- as.factor(data2000$target)

summary(df.scaled.20)
```

### scale (data 1960)

We apply z-score scaling on our 1960s dataset as well, so that two K-Nearest Neighbor models we build for both datasets in the future have the closest characteristic. Then, we can compare the features that involved in two models and try to discover the most important features that did not change through time as well as the features that have changed through time that represent the change of taste of music of the audience through 1960s to 2000s.

```{r scale-data-junhe-60}
# scale tempo ~ liveness
df.scaled.60 <- as.data.frame(scale(data1960[3:9], center = TRUE, scale = TRUE))

df.scaled.60$target <- as.factor(data1960$target)

summary(df.scaled.60)
```

## K-Nearest Neighbor Correlation Matrix

### KNN-correlation matrix of (data 2000)

Before building KNN model, in order to preventing high collinearity between each predictor, it is also necessary to check the correlation between each predictors. From the correlation matrix for 2000s dataset, all predictors are not highly related to each other.

```{r KNN-correlation-junhe-20}
loadPkg('psych')
#pairs(df.scaled.20)
pairs.panels(data2000, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB", # set histogram color, can use "#22AFBB", "red",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )

```

### KNN-correlation matrix of (data 1960)

Here we do the same correlation matrix as before for 1960s model and also looks good. It is time to start building our models.

```{r KNN-correlation-junhe-60}

#pairs(df.scaled.60)
pairs.panels(data1960, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB", # set histogram color, can use "#22AFBB", "red",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
#unloadPkg('psych')
```

## K-Nearest Neighbor Spliting data into train test subsets

### KNN split data into train test subsets (data 2000)

Since datas are very precious, it is always a good idea to split the original dataset into training and testing subsets. Use training data set to fit KNN model and use testing dataset to test the accuracy of our the model is the best way to utilize the dataset. Set seed to 42 so that the result is reproductable, spliting the original data into 4:1 training and testing subset is a very common ratio to use.

```{r train-test-junhe-20}
# split data into train test subset, ratio 4:1, seed 42
seed <- 42
## set the seed to make the partition reproducible
set.seed(seed)
## 80% of the sample size
smp_size <- floor(0.80 * nrow(data2000))
# scale tempo ~ liveness
# df.scaled.20 <- as.data.frame(scale(data2000[3:9], center = TRUE, scale = TRUE))
# df.scaled.20$target <- as.factor(data2000$target)
summary(df.scaled.20)

dat20_sample <- sample(2, nrow(df.scaled.20), replace=TRUE, prob=c(0.80, 0.20))

```

### KNN split data into train test subsets (data 1960)

We do the same spliting process again for our 1960s dataset to keep our entire model building process consistent and comparable. 

```{r train-test-junhe-60}
# split data into train test subset, ratio 4:1, seed 42
seed <- 42
## set the seed to make the partition reproducible
set.seed(seed)
## 80% of the sample size
smp_size <- floor(0.80 * nrow(data1960))
# scale tempo ~ liveness
# df.scaled.20 <- as.data.frame(scale(data2000[3:9], center = TRUE, scale = TRUE))
# df.scaled.20$target <- as.factor(data2000$target)
summary(df.scaled.60)

dat60_sample <- sample(2, nrow(df.scaled.60), replace=TRUE, prob=c(0.80, 0.20))

```

## K-Nearest Neighbor Seperate target variable

### KNN seperate target variable y and predictors X (data 2000)

In order to test accuracy of the fitted models, spliting target variable **target** (1 stands for popular, 0 stands for not popular) variable from the predictors is also necessary.

```{r y-X-junhe-20}
train.X.20 <- df.scaled.20[dat20_sample==1, 1:7]
train.y.20 <- df.scaled.20[dat20_sample==1, 8]
test.X.20 <- df.scaled.20[dat20_sample==2, 1:7]
test.y.20 <- df.scaled.20[dat20_sample==2, 8]
#dim(test.X.20)[1]/dim(df.scaled.20)[1]
summary(test.X.20)
dim(train.X.20)
```

### KNN seperate target variable y and predictors X (data 1960)

We do the same target separation step for 1960s dataset for later accuracy testing as same as the previous step we did for 2000s dataset.

```{r y-X-junhe-60}
train.X.60 <- df.scaled.60[dat60_sample==1, 1:7]
train.y.60 <- df.scaled.60[dat60_sample==1, 8]
test.X.60 <- df.scaled.60[dat60_sample==2, 1:7]
test.y.60 <- df.scaled.60[dat60_sample==2, 8]
#dim(test.X.60)[1]/dim(df.scaled.60)[1]
summary(test.X.60)
dim(train.X.60)
```

## K-Nearest Neighbor helper function to choose best k value

This helper function apply K-Nearest Neighbor model function from **class** library, then build confusion matrix using the predicted target value with the test target value. Finally, using the formula **TP/(TP+NP)** to calculate accuracy of the fitted KNN model with given K value. 

Accuracy is a very important factor to determine the goodness of a fitted KNN model. On the other hand, because of high k-value indicates high precision and high cost on computation, carefully determine the appropriate k-value in order to manage best the trade-off between precision and computation cost is very important. In other word, using the exhaust approach to test a series of k-values and find the most appropriate k-value is the best way to do.

```{r checkK-helper-junhe}
# load class package for knn
loadPkg('class')
# function to selection best k #
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}
```

## K-Nearest Neighbor helper function to selection best combination of features with highest accuracy

In order to build the best KNN model with the given datasets to predict whether a music is popular or not. It is necessary to use exhaust approach to build model with all combinations of predictors and decide which combination of predictors can generate the most accurate model.

The **featureSelectionKNN** helper function applies **combn** function which helps generate a list of **n Choose i** feature combinations, where n stands for number of predictors we have in our dataset in our case is **tempo, key, mode, valence, danceability, energy and liveness** total of 7 features, i stands for number of predictors we want to consider in each iteration from i to n in our case is 1 to 7. Within each iteration, apply **chooseK** function to determine the best k-value for this combination of predictors. Then store the model with the highest accuracy into the result dataframe.

The time complexity for this algorithm is **[(7 Choose 1) times (21-3) times O(knn_function)] +...+ [(7 Choose 7) times (21-3) times O(knn_function)]**. Iterating from **7 Choose 1 to 7 Choose 7**, which is approximately equal to O(n^3), a very time consuming but accurate solution.

```{r featureSelection-junhe}
#features <- combn(names(train.X.20),3)
#features[,1]
#head(train.X.20[,features[,1]])
featureSelectionKNN <- function(train.X, test.X, train.y, test.y){
  res <- data.frame(k = 0, accuracy = 0, features = '')
  for(i in 3:dim(train.X)[2]-1){
    features <- combn(names(train.X),i)
    for(j in 3:dim(features)[2]-1){
      knn_different_k = sapply(seq(3, 21, by = 2),  #<- set k to be odd number from 3 to 21
                         function(x) chooseK(x, 
                                             train_set = train.X[,features[,j]],
                                             val_set = test.X[,features[,j]],
                                             train_class = train.y,
                                             val_class = test.y))
      
      knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])
      
      new.df = data.frame(k = knn_different_k[order(-knn_different_k$accuracy),][1,1],
                          accuracy = knn_different_k[order(-knn_different_k$accuracy),][1,2],
                          features = paste(features[,j],collapse=" ")
                          )
      res = rbind(res, new.df)
    }
    #print(dim(features)[2])
  }
  res
}

```

## KNN feature selection (data 2000)

```{r feature-selection-junhe-20}
features.20 = featureSelectionKNN(train.X.20, test.X.20, train.y.20, test.y.20)
# head(test[order(-test$accuracy),])
f.20 <- data.frame(features.20[order(-features.20$accuracy),])
head(f.20)
```

From the above result, the best KNN model for 2000s data after feature selection has accuracy equal to 0.75 with k-value equal 15 and with 5 features which are **tempo, valence, danceability, energy and liveness**.

The alternative option is the second best KNN model with accuracy equal to 0.75 as same as the best model but with lower k-value which equals 13, and 6 instead of 5 features which are **tempo, mode, valence, danceability, energy and liveness**. However, high k-value can cause expensive computation cost and second best KNN model seems to be better than the best KNN model in the list, we still want to minimize the number of involved predictors to be able to explore the most important factors that decide whether a music is popular or not in 2000s.

## KNN feature selection (data 1960)

```{r feature-selection-junhe-60}
features.60 = featureSelectionKNN(train.X.60, test.X.60, train.y.60, test.y.60)
# head(test[order(-test$accuracy),])
f.60 <- data.frame(features.60[order(-features.60$accuracy),])
head(f.60)
```

From the above result, the best KNN model for 1960s data after feature selection has accuracy equal to 0.68 with k-value equal 19 and has only 4 features which are **mode, valence, energy and liveness**.

The alternative option is the second best KNN model with accuracy equal to 0.68 as same as the best model but with higher k-value which equals 21, and 5 instead of 4 features which are **key, mode, valence, danceability, energy**. This time the second best KNN model loses on both k-value and number of predictors. So, we choose the best model without doubt as our final KNN model for 1960s dataset.

In conclusion, comparing the best model for 1960s dataset and 2000s dataset, the important features to determine whether a music is popular has changed from **mode, valence, energy and liveness** in 1960s to **tempo, valence, danceability, energy and liveness** in 2000s. In other word, people in 1960s took **mode, valence, energy and liveness** these four music elements as the most important features to determine whether a music is good or not, and has changed in 2000s to **tempo, valence, danceability, energy and liveness** these five music elements. People still think **valence, energy and liveness** are important factors but no longer took **mode** as an important factor. Moreover, took **tempo and danceability** as two new important factors.

##Visualize k-values in the best KNN model (data 2000)

After spotting the best features combination and k-values, it is time for us to start building our KNN model using 2000s dataset with the best subset of features and check correctness of our above **featureSelectionKNN** result. Notice that the best k-value among range 1 through 81 is still 15 which is as same as the result we get from **featureSelectionKNN** function. On the other hand, k-value equal 15 is also the ankle value of the k-values series which also demonstrate that this is the best k-value we are looking for.

```{r KNN-junhe-20}
# features selected by features selection function, with the best accuracy
best_features.20 <- c('tempo', 'valence', 'danceability', 'energy', 'liveness')

knn_different_k = sapply(seq(1, 81, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = train.X.20[,best_features.20],
                                             val_set = test.X.20[,best_features.20],
                                             train_class = train.y.20,
                                             val_class = test.y.20))

# Reformat the results to graph the results.
#str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg('ggplot2')

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
#summary(pima$type)[2]/dim(pima)[1]
#head(knn_different_k[order(knn_different_k$accuracy),])
#as.data.frame(knn_different_k[order(-knn_different_k$accuracy),])[1,2]
```

##Visualize k-value trend in the best KNN model (data 1960)

Same way to build our KNN model using 1960s dataset with the best subset of features and check correctness our above **featureSelectionKNN** result. Notice that the best k-value among 1 through 81 is still 19 which is as same as the result we get from **featureSelectionKNN** function. On the other hand, k-value equal 19 is also the ankle value of the k-values series which also demonstrate that this is the best k-value we are looking for.

```{r KNN-junhe-60}
# features selected by features selection function, with the best accuracy
best_features.60 <- c('mode', 'valence', 'energy', 'liveness')

knn_different_k = sapply(seq(1, 81, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = train.X.60[,best_features.60],
                                             val_set = test.X.60[,best_features.60],
                                             train_class = train.y.60,
                                             val_class = test.y.60))

# Reformat the results to graph the results.
#str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg('ggplot2')

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
#summary(pima$type)[2]/dim(pima)[1]
#head(knn_different_k[order(knn_different_k$accuracy),])
#as.data.frame(knn_different_k[order(-knn_different_k$accuracy),])[1,2]
```

The correctness of all the results has now been proved


```{r uzscale_fcn, include=F}
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
  
  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
```
```{r Krystal_packageload, include=F}
loadPkg(xtable)
loadPkg(kableExtra)
loadPkg(stringi)
```

# Decision Tree Analysis - Krystal

The main goal of this section is to predict whether the target class of popular and non-popular songs will have different variables that affect them in the 1960s and 2000s.  Will the songs have the same variables that matter most in both decades or will they differ? 

## Grow the Tree
In this section, the study will be using the Categorical Decision Tree to show what makes a song popular in the 1960s verses the 2000s.  The variables of tempo, key, valence, danceability, energy, liveness, and mode are the independent variables and target will be used as the dependent variable.  First there had to be changings made to the variable target so it could be a factor variable.  Once this has been done, one can proceed to grow the decision tree by starting at the root and building up.  Our root will be target which has already been stated as the dependent variable:

```{r cleandata, include=F}
data1960$target <- as.factor(data1960$target)
str(data1960)
data2000$target <- as.factor(data2000$target)
str(data2000)
```

```{r KPTree1960, echo = T, fig.dim=c(6,4)}
set.seed(1)
data1960_DT <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data1960, method="class", control = list(maxdepth = 10) )
printcp(data1960_DT) # display the results 
plotcp(data1960_DT) # visualize cross-validation results 
summary(data1960_DT) # detailed summary of splits
# plot tree 
plot(data1960_DT, uniform=TRUE, main="Classification Tree for 1960 Songs")
text(data1960_DT, use.n=TRUE, all=TRUE, cex=.8)
```

```{r KPTree2000, echo = T, fig.dim=c(6,4)}
set.seed(1)
data2000_DT <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data2000, method="class", control = list(maxdepth = 10) )
printcp(data2000_DT) # display the results 
plotcp(data2000_DT) # visualize cross-validation results 
summary(data2000_DT) # detailed summary of splits
# plot tree 
plot(data2000_DT, uniform=TRUE, main="Classification Tree for 2000 Songs")
text(data2000_DT, use.n=TRUE, all=TRUE, cex=.8)
```


```{r Postscript, results=T}
# create attractive postcript plot of tree 
post(data1960_DT, file = "data1960.ps", title = "Classification Tree for 1960s Song List")
post(data2000_DT, file = "data2000.ps", title = "Classification Tree for 2000s Song List")
```

Here are the results. 

For the 1960s Song List:
From the decision tree, the first branching point has 4321 popular and 4321 unpopular songs from the data. The first split yields 2015 outcomes with energy levels < 0.27 and 6627 outcomes with energy levels >= 0.27 for the popularity results for the song list.  The energy level was showns as being the most important variable to determine whether the song was popular or not.  The energy level being under 0.27 represents that the song was unpopular and the energy level greater than 0.27 deems the song as popular.  The variable that follows after the energy level was the variance level.  There were only two variables that determined the popularity of the songs making the Billboard Top 100 list. For the song to be considered popular, the energy of the song has to be equal to or greater than 0.27.  A further split on the left node might not have been beneficial to separate the popular and unpopular songs list. The algorithm actually stops there, and predicts unpopular for all 2015 observations, giving 476/2015 incorrect predictions. 

Nonethelss, the lower branch continue to split, with  (67%) correct prediction on the last leave (node at end of branch) and 481/772 correct **non popular** prediction on the third leaf, while the last leaf predicted 3604/5905 **popular** correctly. Not too bad.

Overall, when the model predicts **unpopular**, accuracy is (1539+481)/(1539+476+481+291) = `r round( (1539+481)/(1539+476+481+291)*100,1 )`%. And when the model predicts **popular**, accuracy is (29+12+12)/(29+0+12+0+12+2) = `r round( (29+12+12)/(29+0+12+0+12+2)*100,1 )`%.

Now the 2000s Song List can be summarized:
The decision tree starts the first branching point with 2936 popular and 2936 unpopular songs from the data presented from Spotify. The first split yields 1319 outcomes with danceability levels < 0.396 and 4553 outcomes with danceability levels >= 0.396 for the popularity results for the song list.  The danceability levels has been identified as the most important variable to determine whether the song was popular or not in the 2000s.  Danceability levels being under 0.396 represents that the song was unpopular while the danceability levels being greater than 0.396 are deemed as the song being popular.  The variables that follows after the danceability level are the energy and then valence levels.  Unlike the 1960s, there are three variables that determined the popularity of the songs making the Billboard Top 100 list. For the song to be considered popular, the danceability of the song has to be equal to or greater than 0.396.  A further split on the left node might not have been beneficial just like in the 1960s decision tree to separate the popular and unpopular songs list. The algorithm actually stops there, and predicts unpopular for all 1319 observations, giving 157/2015 incorrect predictions. 

Nonethelss, the lower branch continue to split, with  (60%) correct prediction on the last leaf (node at end of branch) and 129/214 correct **non popular** prediction on the third leaf, while the last leaf predicted 609/976 **popular** correctly.

Overall, when the model predicts **unpopular**, accuracy is (1539+481)/(1539+476+481+291) = `r round( (1539+481)/(1539+476+481+291)*100,1 )`%. And when the model predicts **popular**, accuracy is (29+12+12)/(29+0+12+0+12+2) = `r round( (29+12+12)/(29+0+12+0+12+2)*100,1 )`%.

From the decision tables above we see that what made a song popular in the 1960s did not necessarily make a song popular in the 2000s.  They had one differing variable that made the difference.  In the 2000s, the most important variable for the song to make the Billboards Top 100 list was for the song to have danceability.  Songs like Hot in Herre by Nelly is a song that has the danceability that is talked about.  In the 1960s, a song needed to have energy as the most impoartant variable.  We can check our decision tables but creating confusion matrixes.

These perentages for both the 1960s and 2000s songs can represent by the calculations below that are being examined in confusion matrixes.


```{r KPConTab1960, results=T}
loadPkg(caret) 
cm_1960 = confusionMatrix( predict(data1960_DT, type = "class"), reference = data1960[, "target"] )
print('Overall: ')
cm_1960$overall
print('Class: ')
cm_1960$byClass
unloadPkg(caret)
```

```{r KPTable1960, results="asis"}
xkabledply(cm_1960$table, "confusion matrix")
```

```{r KPConTable2000, results=T}
loadPkg(caret) 
cm_2000 = confusionMatrix( predict(data2000_DT, type = "class"), reference = data2000[, "target"] )
print('Overall: ')
cm_2000$overall
print('Class: ')
cm_2000$byClass
unloadPkg(caret)
```


```{r KPTable2000, results="asis"}
xkabledply(cm_2000$table, "confusion matrix")
```
Now that the calculations are calculated in the confusion matrix, another method can be used and it uses the maxdepth with the results being put into a summary below.


1960s Song Max Depth Summary:
```{r KPmaxdepth1960}
loadPkg(rpart)
loadPkg(caret)
# create an empty dataframe to store the results from confusion matrices
confusionMatrixResultDf_1 = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )
for (deep in 2:6) {
  fit1960 <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data1960, method="class", control = list(maxdepth = deep) )
  # 
  cm_1 = confusionMatrix( predict(fit1960, type = "class"), reference = data1960[, "target"] ) # from caret library
  # 
  cmaccu_1 = cm_1$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt_1 = data.frame(Depth=deep, Accuracy = cmaccu_1, row.names = NULL ) # initialize a row of the metrics 
  cmt_1 = cbind( cmt_1, data.frame( t(cm_1960$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf_1 = rbind(confusionMatrixResultDf_1, cmt_1)
  # print("Other metrics : ")
}
unloadPkg(caret)
```

The summarized result is here:

```{r TreeClass, results="asis"}
xkabledply(confusionMatrixResultDf_1, "1960s Song List Classification Trees summary with varying MaxDepth")
```

2000s Song Max Depth Summary:
```{r KPmaxdepth2000}
loadPkg(rpart)
loadPkg(caret)
# create an empty dataframe to store the results from confusion matrices
confusionMatrixResultDf_2 = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )
for (deep in 2:6) {
  fit2000 <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data2000, method="class", control = list(maxdepth = deep) )
  # 
  cm_2 = confusionMatrix( predict(fit2000, type = "class"), reference = data2000[, "target"] ) # from caret library
  # 
  cmaccu_2 = cm_2$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt_2 = data.frame(Depth=deep, Accuracy = cmaccu_2, row.names = NULL ) # initialize a row of the metrics 
  cmt_2 = cbind( cmt_2, data.frame( t(cm_2$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf_2 = rbind(confusionMatrixResultDf_2, cmt_2)
  # print("Other metrics : ")
}
unloadPkg(caret)
```

The summarized result is here:

```{r TreeClass2, results="asis"}
xkabledply(confusionMatrixResultDf_2, "2000s Song List Classification Trees summary with varying MaxDepth")
```


The following are two additional ways that we can plot out the categorical decision tree.  The additional methods are the 'rpart.plot' and 'fancy' plots to help read the tree easier.  Both the 1960s and 2000s songs have been placed into the models as follows:


1960s Song Fancy and Rpart Plots
```{r KPfancyplot_1960, results=T}
loadPkg("rpart.plot")
rpart.plot(data1960_DT)
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
fancyRpartPlot(data1960_DT)
```

2000s Song Fancy and Rpart Plots
```{r KPfancyplot_2000, results=T}
loadPkg("rpart.plot")
rpart.plot(data2000_DT)
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
fancyRpartPlot(data2000_DT)
```


## Prune the tree

We can now prune the decision trees to get the most important results to know what variables are most important to be put onto the Billboards Top 100 list.  The following are the pruning of the trees:

```{r KPprune1960, results=T}
#prune the tree 
PT_data1960_DT <- prune(data1960_DT, cp = data1960_DT$cptable[2,"CP"])
#pkyphosisfit <- prune(kyphosisfit, cp = kyphosisfit$cptable[which.min( kyphosisfit$cptable[,"xerror"] ),"CP"])
# plot the pruned tree 
fancyRpartPlot(PT_data1960_DT)
# For boring plot, use codes below instead
plot(PT_data1960_DT, uniform=TRUE, main="Pruned Classification Tree for 1960s Song List")
text(PT_data1960_DT, use.n=TRUE, all=TRUE, cex=.8)
```


```{r KPprune2000, results=T}
#prune the tree 
PT_data2000_DT <- prune(data2000_DT, cp = data2000_DT$cptable[2,"CP"])
#pkyphosisfit <- prune(kyphosisfit, cp = kyphosisfit$cptable[which.min( kyphosisfit$cptable[,"xerror"] ),"CP"])
# plot the pruned tree 
fancyRpartPlot(PT_data2000_DT)
# For boring plot, use codes below instead
plot(PT_data2000_DT, uniform=TRUE, main="Pruned Classification Tree for 2000s Song List")
text(PT_data2000_DT, use.n=TRUE, all=TRUE, cex=.8)
```

From the pruned trees, the results show different variables for each year on what classified them to get on the Billboards Top 100 List.  For the 1960s songs, the song needed to have an energy level greater than 0.27.  This is shown as the most independent variable that determines the popularity of a song.  In order for a song in the 2000s to make the list, the danceability matter more than energy.  The dancebility had to be greater than a level of 0.40.  In the two eras, there were differences in what was deemed as most important.  In the 1960s, a person needed to have the right energy being eluded from the music while in the 2000s the song had to be a danceable song.

# Logit Regression - Siwei

Logistic regression is a type of generalized linear model that uses statistical analysis to predict an event based on known factors. I use logistic regression here to make predictions about whether a song will be popular in different years based on features we used before like danceability, tempo, valence and energy.

I change some variables into categorical first to make better analysis later on.

```{r LogReg-Siwei}
data1960$track <- factor(data1960$track)
data1960$artist <- factor(data1960$artist)
data1960$key <- factor(data1960$key)
data1960$mode <- factor(data1960$mode)
data1960$target <- factor(data1960$target)
```

```{r LogReg-Siwei}
data1960Logit_1 <- glm(target ~ tempo, data = data1960, family = "binomial")
data1960Logit_1
```

We can see the summary of the logit model here:  
```{r logitSummary-Siwei}
summary(data1960Logit_1)
```



```{r LogReg-Siwei}
data1960Logit_2 <- glm(target ~ tempo+valence, data = data1960, family = "binomial")
data1960Logit_2
summary(data1960Logit_2)
```


```{r LogReg-Siwei}
data1960Logit_3 <- glm(target ~ tempo+valence+danceability, data = data1960, family = "binomial")
data1960Logit_3
summary(data1960Logit_3)
```


```{r LogReg-Siwei}
data1960Logit_4 <- glm(target ~ tempo+valence+danceability+energy, data = data1960, family = "binomial")
data1960Logit_4
summary(data1960Logit_4)
```

```The best model for year 1960 is data1960Logit_4. All the coefficients are found significant because of the small p-values. Tempo, valence, danceability and energy all have positive effects on music popularity. Energy had the biggest positive coefficient on music in 1960, while tempo had the smallest influence on popularity.```
```{r}
x = exp(coef(data1960Logit_4)) 
x
```


```{r siwei-logit_fitted_value}
data1960Logit$fitted.values[1]
predict(data1960Logit)[1]
1/(1+exp(-predict(data1960Logit)[1]))
```

### Model evaluation
#### Confusion matrix 

Confusion matrix is a table that is often used to describe the performance of a classifier on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. 

This is just one of the many libraries you can find the confusion matrix. It is easy to use, but not very powerful, lacking ability to choose cutoff value, and it does not give you all the metrics like accuracy, precision, recall, sensitivity, f1 score etc. 


```{r confusionMatrix}
loadPkg(regclass)
confusion_matrix(admitLogit)
unloadPkg(regclass)
```


```{r LogReg-Siwei}
data1960Logit_5 <- glm(target ~ tempo+valence+danceability+energy+liveness, data = data1960, family = "binomial")
data1960Logit_5
summary(data1960Logit_5)
```


#### Hosmer and Lemeshow test  

The Hosmer and Lemeshow Goodness of Fit test can be used to evaluate logistic regression fit.

```{r HosmerLemeshow-Siwei}
loadPkg(ResourceSelection) # function hoslem.test( ) for logit model evaluation
data1960Logit_4Hoslem = hoslem.test(data1960$target, fitted(data1960Logit_1)) # Hosmer and Lemeshow test, a chi-squared test
unloadPkg(ResourceSelection) 
```

The result is shown here:  
```{r HosmerLemeshowRes-Siwei, results='markup', collapse=F}
data1960Logit_4Hoslem
```

H0: no significant difference between the model and the observed data

p-value < 0.05, we can reject the null, which indicates the model is a good fit, at the same time all the coefficients are significant.

Similar to comparing the actual frequency distribution to the model predicted frequency distribution. 
 
Of course this is just a very simple simulation study, but it is nice to see that, at least for the setup we have used, the test performs as we would hope.


#### Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC)

ROC and AUC measures the true positive rate (or sensitivity) against the false positive rate (or specificity). The area-under-curve is always between 0.5 and 1. Values higher than 0.8 is considered good model fit.  
```{r roc_auc}
loadPkg(pROC) 
prob=predict(data1960Logit, type = c("response"))
data1960Logit$prob=prob
h <- roc(target~prob, data=data1960)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg(pROC) 
```


We have here the area-under-curve of `r auc(h)`, which is less than 0.8. This test also agrees the model is not considered tht good fit,but it's okay for the fitting. 

#### McFadden  

McFadden is another evaluation tool we can use on logit regressions. This is part of what is called pseudo-R-squared values for evaluation tests. 

```{r McFadden_direct}
data1960NullLogit <- glm(target ~ 1, data = data1960, family = "binomial")
mcFadden = 1 - logLik(data1960Logit)/logLik(data1960NullLogit)
mcFadden
```

Or we can use other libraries. The `pscl` (Polictical Science Computational Lab) library has the function `pR2()` (pseudo-R$^2$) will do the trick.  

```{r McFadden}
loadPkg(pscl) # use pR2( ) function to calculate McFadden statistics for model eval
data1960Logitpr2 = pR2(data1960Logit)
data1960Logitpr2
unloadPkg(pscl) 
```

With the McFadden value of the model, which is analgous to the coefficient of determination $R^2$, only about 8% of the variations in y is explained by the explanatory variables in the model. 

A major weakness of the overall model is likely from the small dataset sample size of `r length(Admit$admit)`. We expect a much higher number of observations will increase the sensitivity of the model.





```{r LogReg-Siwei}
data2000$track <- factor(data2000$track)
data2000$artist <- factor(data2000$artist)
data2000$key <- factor(data2000$key)
data2000$mode <- factor(data2000$mode)
data2000$target <- factor(data2000$target)
```


```{r LogReg-Siwei}
data2000Logit_1 <- glm(target ~ tempo, data = data2000, family = "binomial")
summary(data2000Logit_1)
```


```{r LogReg-Siwei}
data2000Logit_2 <- glm(target ~ tempo+valence, data = data2000, family = "binomial")
summary(data2000Logit_2)
```


```{r LogReg-Siwei}
data2000Logit_3 <- glm(target ~ tempo+valence+danceability, data = data2000, family = "binomial")
summary(data2000Logit_3)
```


```{r LogReg-Siwei}
data2000Logit_4 <- glm(target ~ tempo+valence+danceability+energy, data = data2000, family = "binomial")
summary(data2000Logit_4)
```


```{r LogReg-Siwei}
data2000Logit_5 <- glm(target ~ tempo+valence+danceability+energy+liveness, data = data2000, family = "binomial")
summary(data2000Logit_5)
```

```The best model for year 2000 is data2000Logit_5. From the p value, only danceability, energy and liveness coefficients are found significant. Tempo, danceability and energy have positive effects on popularity. On the contract, valence and liveness have negative effects. ```
```{r}
x = exp(coef(data2000Logit_5)) 
x
```


#### Hosmer and Lemeshow test  

The Hosmer and Lemeshow Goodness of Fit test can be used to evaluate logistic regression fit.

```{r HosmerLemeshow}
loadPkg(ResourceSelection) # function hoslem.test( ) for logit model evaluation
data2000Logit_5Hoslem = hoslem.test(data2000$target, fitted(data2000Logit_5)) # Hosmer and Lemeshow test, a chi-squared test
unloadPkg(ResourceSelection)
```

The result is shown here:  
```{r HosmerLemeshowRes, results='markup', collapse=F}
data2000Logit_5Hoslem
```


``` The p-value of `r data2000Logit_5Hoslem$p.value` is quite samll. This indicates the model is a good fit, despite only three coefficients are found significant. ```
#### Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC)
ROC and AUC measures the true positive rate (or sensitivity) against the false positive rate (or specificity). The area-under-curve is always between 0.5 and 1. Values higher than 0.8 is considered good model fit.  
```{r roc_auc}
loadPkg(pROC) 
prob=predict(data2000Logit_5, type = c("response"))
data2000$prob=prob
h <- roc(target~prob, data=data2000)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg(pROC) 
```


We have here the area-under-curve of `r auc(h)`, which is almost 0.8. This test shows that the model is a good fit. 

#### McFadden  

McFadden is another evaluation tool we can use on logit regressions. This is part of what is called pseudo-R-squared values for evaluation tests. We can calculate the value directly from its definition if we so choose to.

```{r McFadden_direct}
data2000NullLogit <- glm(target ~ 1, data = data2000, family = "binomial")
mcFadden = 1 - logLik(data2000Logit_5)/logLik(data2000NullLogit)
mcFadden
```

Or we can use other libraries. The `pscl` (Polictical Science Computational Lab) library has the function `pR2()` (pseudo-R$^2$) will do the trick.  

```{r McFadden}
loadPkg(pscl) # use pR2( ) function to calculate McFadden statistics for model eval
data2000Logitpr2 = pR2(data2000Logit_5)
data2000Logitpr2
unloadPkg(pscl) 
```


A major weakness of the overall model is likely from the small dataset sample size. We expect a much higher number of observations will increase the sensitivity of the model.


### Model evaluation
#### Confusion matrix 

This is just one of the many libraries you can find the confusion matrix. It is easy to use, but not very powerful, lacking ability to choose cutoff value, and it does not give you all the metrics like accuracy, precision, recall, sensitivity, f1 score etc. Nonetheless, it's handy.

```{r confusionMatrix}
loadPkg(regclass)
confusion_matrix(admitLogit)
unloadPkg(regclass)
```


# Summary of Results



---
[^1] Taken from [Spotify for Developers Web Services Page](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/)
[^2] From Molanphy, Chris (August 1, 2013). "How The Hot 100 Became America's Hit Barometer". Published by NPR.
